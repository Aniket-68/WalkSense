# WalkSense Quick Start

## ğŸš€ Fast Setup (5 minutes)

### 1. Automated Setup

**Linux/Mac:**
```bash
cd WalkSense
./setup.sh
```

**Windows:**
```cmd
cd WalkSense
setup.bat
```

This will:
- âœ… Create virtual environment
- âœ… Install Python dependencies
- âœ… Download YOLO model
- âœ… Test camera and components

### 2. Install LM Studio

1. Download from **[lmstudio.ai](https://lmstudio.ai)**
2. Install and launch
3. Search for `Qwen2-VL-2B-Instruct-GGUF`
4. Download the model (Q4 or Q5 version)
5. Click **"Local Server"** tab â†’ **"Start Server"**

### 3. Run WalkSense

**Enhanced version (with LLM):**
```bash
python scripts/run_enhanced_camera.py
```

**Basic version (no LLM needed):**
```bash
python scripts/run_camera.py
```

---

## âŒ¨ï¸ Controls

| Key | Action |
|-----|--------|
| `S` | **Start** system |
| `L` | **Ask question** (push-to-talk) |
| `M` | **Mute/Unmute** audio |
| `Q` | **Quit** |

---

## ğŸ¯ Quick Test

After running, you should see:

1. **Camera window opens** showing live feed
2. **YOLO detections** with colored bounding boxes
3. **Spatial tracking** info at top (`Tracking: car center, person left`)
4. **Audio announcements** for detected objects

**Try:**
- Wave an object in front of camera â†’ should announce "object detected"
- Press `L` and ask "What do you see?" â†’ LLM answers with context

---

## ğŸ”§ Common Issues

### "LM Studio connection failed"
â†’ Make sure LM Studio is running and server started on port 1234

### "Camera not working"
â†’ Check camera permissions, try different camera ID in code

### "YOLO model not found"
â†’ Run setup script again or manually download:
```bash
python -c "from ultralytics import YOLO; YOLO('yolov8n.pt')"
mv yolov8n.pt models/yolo/
```

### "Import errors"
â†’ Make sure you're in WalkSense directory:
```bash
cd /path/to/WalkSense
source venv/bin/activate  # or venv\Scripts\activate on Windows
```

---

## ğŸ“‚ Project Structure

```
WalkSense/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_camera.py              # Basic demo
â”‚   â””â”€â”€ run_enhanced_camera.py     # Enhanced with LLM
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ spatial_context_manager.py # Object tracking (NEW)
â”‚   â”œâ”€â”€ llm_reasoner.py           # LLM integration (NEW)
â”‚   â””â”€â”€ fusion_engine.py          # Enhanced pipeline
â”œâ”€â”€ safety/
â”‚   â”œâ”€â”€ yolo_detector.py          # YOLO detection
â”‚   â””â”€â”€ safety_rules.py           # Hazard classification
â””â”€â”€ models/
    â””â”€â”€ yolo/
        â””â”€â”€ yolov8n.pt            # YOLO weights
```

---

## ğŸ“ Learn More

- **`SETUP_GUIDE.md`** - Detailed setup instructions
- **`ENHANCED_SYSTEM.md`** - Feature documentation
- **`docs/API_EXAMPLES.md`** - Code examples
- **`docs/PIPELINE_FLOW.md`** - Architecture diagrams

---

## ğŸ’¡ Example Questions to Ask

Once running, press `L` and try:

- â“ "What's in front of me?"
- â“ "Is it safe to move forward?"
- â“ "What's on my left?"
- â“ "Describe what you see"
- â“ "Are there any obstacles?"

The LLM will answer using:
- Current object positions (from spatial tracking)
- Scene understanding (from VLM)
- Recent movement events

---

## ğŸ—ï¸ Architecture Overview

```
Camera â†’ YOLO â†’ Spatial Tracker â†’ Context Manager
                     â†“
                VLM Scene Description
                     â†“
              Context Memory
                     â†“
User Query â†’ LLM Reasoner â†’ Answer â†’ TTS
```

**Key Features:**
- ğŸ¯ **Object Tracking**: Maintains identity across frames
- ğŸ“ **Spatial Awareness**: Knows position, direction, distance
- ğŸ§  **LLM Reasoning**: Answers questions intelligently
- ğŸš¨ **Safety First**: Critical alerts always override

---

## ğŸ”„ Workflow Example

1. **Frame 1**: Camera detects car
   - ğŸ¯ Tracker assigns ID: `track_42`
   - ğŸ“¢ Announces: "car detected on center"

2. **Frame 50**: Same car moved
   - ğŸ¯ Tracker: "car moved 45px"
   - ğŸ“¢ Announces: "car moving center"

3. **User asks**: "Is it safe?"
   - ğŸ§  LLM gets context: "car center at close distance"
   - ğŸ§  LLM gets VLM: "Parking lot with vehicles"
   - ğŸ“¢ Answers: "Caution, car close ahead"

---

## ğŸ“Š Performance

- **YOLO**: 30-50ms per frame
- **Spatial Tracking**: ~5ms (negligible)
- **VLM**: 2-5s (async, non-blocking)
- **LLM Query**: 1-3s

Total real-time loop: **~30-50ms** âœ…

---

## ğŸ›ï¸ Tuning

Edit in `run_enhanced_camera.py`:

```python
# Process VLM every N frames (default: 150)
sampler = FrameSampler(every_n_frames=150)

# Scene change threshold (default: 15%)
scene_detector = SceneChangeDetector(threshold=0.15)

# Spatial tracking sensitivity
SpatialContextManager(
    movement_threshold=30.0,  # pixels
    time_threshold=10.0       # seconds
)
```

**Lower values** = More sensitive, more announcements
**Higher values** = Less sensitive, fewer announcements

---

## ğŸ†˜ Need Help?

1. Check `SETUP_GUIDE.md` for detailed troubleshooting
2. Verify LM Studio is running
3. Test components individually (see setup scripts)
4. Check console for error messages
