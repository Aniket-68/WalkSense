"""
SystemManager: Thread-safe singleton that owns the WalkSense processing pipeline.
Replaces the OpenCV main loop from run_enhanced_camera.py with a headless server-side loop.
"""

import cv2
import time
import threading
import queue
from typing import Optional, Dict, Any, List
from loguru import logger
from dataclasses import dataclass, field


class SystemManager:
    """Central manager for the WalkSense processing pipeline.
    
    Runs the camera → YOLO → safety → VLM → LLM pipeline in a background thread
    and exposes state via get_state() for the WebSocket API.
    """

    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
            return cls._instance

    def __init__(self):
        if self._initialized:
            return
        self._initialized = True

        # State
        self._running = False
        self._system_status = "IDLE"
        self._loop_thread: Optional[threading.Thread] = None

        # Pipeline component states
        self._pipeline_state = {
            "camera":  {"active": False, "last_latency_ms": 0},
            "yolo":    {"active": False, "detections_count": 0, "last_latency_ms": 0},
            "safety":  {"active": False, "last_alert": None},
            "vlm":     {"active": False, "is_processing": False, "last_latency_ms": 0},
            "llm":     {"active": False, "is_processing": False, "last_latency_ms": 0},
            "tts":     {"active": False, "is_speaking": False},
            "stt":     {"active": False, "is_listening": False},
        }

        # Data
        self._latest_description = ""
        self._spatial_summary = ""
        self._current_query: Optional[str] = None
        self._current_response: Optional[str] = None
        self._dialogue_history: List[Dict[str, Any]] = []
        self._detections: list = []

        # Frame buffer
        self._frame_lock = threading.Lock()
        self._latest_frame: Optional[bytes] = None  # JPEG bytes
        self._latest_raw_frame = None  # numpy array for VLM

        # Query queue
        self._query_queue: queue.Queue = queue.Queue()

        # Lazy-loaded components (initialized on start)
        self._camera = None
        self._detector = None
        self._safety = None
        self._tts = None
        self._fusion = None
        self._qwen = None
        self._sampler = None
        self._scene_detector = None
        self._qwen_worker = None
        self._muted = False

        logger.info("[SystemManager] Initialized (singleton)")

    def _init_components(self):
        """Lazy-initialize all pipeline components."""
        import os
        from infrastructure.config import Config
        from perception_layer.camera import Camera
        from perception_layer.detector import YoloDetector
        from perception_layer.rules import SafetyRules
        from fusion_layer.engine import FusionEngine
        from interaction_layer.tts import TTSEngine
        from reasoning_layer.vlm import QwenVLM
        from infrastructure.sampler import FrameSampler
        from infrastructure.scene import SceneChangeDetector

        # VLM config
        vlm_provider = Config.get("vlm.active_provider", "lm_studio")
        vlm_config_path = f"vlm.providers.{vlm_provider}"
        vlm_url = Config.get(f"{vlm_config_path}.url")
        vlm_model = Config.get(f"{vlm_config_path}.model_id")

        # LLM config
        llm_provider = Config.get("llm.active_provider", "ollama")
        llm_config_path = f"llm.providers.{llm_provider}"
        llm_url = Config.get(f"{llm_config_path}.url", "")
        llm_model = Config.get(f"{llm_config_path}.model_id")
        
        # API key for Gemini (read from env var specified in config)
        llm_api_key = None
        if llm_provider == "gemini":
            api_key_env = Config.get(f"{llm_config_path}.api_key_env", "GEMINI_API_KEY")
            llm_api_key = os.getenv(api_key_env)
            if not llm_api_key:
                logger.warning(f"[SystemManager] Gemini API key not found in env var: {api_key_env}")

        # Perception thresholds
        sampling = Config.get("perception.sampling_interval", 150)
        scene_thresh = Config.get("perception.scene_threshold", 0.15)

        logger.info("[SystemManager] Initializing pipeline components...")

        self._camera = Camera()
        self._detector = YoloDetector()
        self._safety = SafetyRules()
        self._tts = TTSEngine()

        self._fusion = FusionEngine(
            self._tts,
            llm_backend=llm_provider,
            llm_url=llm_url,
            llm_model=llm_model,
            llm_api_key=llm_api_key
        )

        self._qwen = QwenVLM(
            backend=vlm_provider,
            model_id=vlm_model,
            lm_studio_url=vlm_url
        )

        self._sampler = FrameSampler(every_n_frames=sampling)
        self._scene_detector = SceneChangeDetector(threshold=scene_thresh)

        # VLM worker (async VLM inference in background thread)
        self._qwen_worker = _QwenWorker(self._qwen)

        # LLM worker (async LLM inference in background thread)
        self._llm_worker = _LLMWorker(self._fusion)

        # Pre-load Whisper STT model
        self._preload_whisper(Config)

        logger.info("[SystemManager] All components initialized")

    def _preload_whisper(self, Config):
        """Pre-load Whisper model during startup to avoid lag on first voice query."""
        import os
        try:
            provider = Config.get("stt.active_provider", "whisper_local")
            config_path = f"stt.providers.{provider}"
            model_size = Config.get(f"{config_path}.model_size", "base")
            device = Config.get(f"{config_path}.device", "cuda")
            compute_type = Config.get(f"{config_path}.compute_type", "int8")

            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            model_dir = os.path.join(project_root, "models", "whisper")

            logger.info(f"[STT] Pre-loading Whisper model '{model_size}' on {device}...")
            
            # ENFORCE GPU - No CPU fallback
            if device == "cuda":
                try:
                    from faster_whisper import WhisperModel
                    self._whisper_model = WhisperModel(
                        model_size, device=device, compute_type=compute_type,
                        download_root=model_dir
                    )
                    self._whisper_backend = "faster_whisper"
                    logger.info(f"[STT] ✓ Faster-Whisper loaded successfully on CUDA ({model_size})")
                    return
                except Exception as e:
                    logger.error(f"[STT] ✗ CUDA REQUIRED but failed: {e}")
                    logger.error("[STT] ═══════════════════════════════════════")
                    logger.error("[STT] GPU ENFORCEMENT: Install CUDA PyTorch:")
                    logger.error("[STT] pip uninstall torch torchvision torchaudio")  
                    logger.error("[STT] pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
                    logger.error("[STT] ═══════════════════════════════════════")
                    raise RuntimeError(f"CUDA required but unavailable: {e}")

            # CPU only allowed if explicitly configured
            elif device == "cpu":
                from faster_whisper import WhisperModel
                self._whisper_model = WhisperModel(
                    model_size, device="cpu", compute_type="int8",
                    download_root=model_dir
                )
                self._whisper_backend = "faster_whisper"
                logger.info(f"[STT] Faster-Whisper loaded on CPU ({model_size}) - GPU disabled by config")
            else:
                raise ValueError(f"Invalid device: {device}")
                
        except Exception as e:
            logger.error(f"[STT] Pre-load failed: {e}")
            self._whisper_model = None
            self._whisper_backend = None
            raise  # Re-raise to stop initialization

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def start(self):
        """Start the processing pipeline."""
        if self._running:
            return

        if self._camera is None:
            self._init_components()

        self._running = True
        self._system_status = "RUNNING"
        self._loop_thread = threading.Thread(target=self._main_loop, daemon=True)
        self._loop_thread.start()
        logger.info("[SystemManager] Pipeline started")

    def stop(self):
        """Stop the processing pipeline."""
        self._running = False
        self._system_status = "IDLE"

        # Reset pipeline states
        for key in self._pipeline_state:
            self._pipeline_state[key]["active"] = False
            if "is_processing" in self._pipeline_state[key]:
                self._pipeline_state[key]["is_processing"] = False

        if self._qwen_worker:
            self._qwen_worker.stop()

        if hasattr(self, '_llm_worker') and self._llm_worker:
            self._llm_worker.stop()

        logger.info("[SystemManager] Pipeline stopped")

    def submit_query(self, text: str):
        """Submit a user text query."""
        self._query_queue.put(text)
        self._current_query = text
        self._pipeline_state["stt"]["active"] = True
        self._dialogue_history.append({
            "role": "user",
            "text": text,
            "timestamp": time.time()
        })
        logger.info(f"[SystemManager] Query submitted: {text}")

    def toggle_mute(self) -> bool:
        """Toggle audio mute."""
        if self._fusion:
            self._muted = self._fusion.router.toggle_mute()
        return self._muted

    def transcribe_audio(self, audio_bytes: bytes) -> Optional[str]:
        """Transcribe audio bytes (WAV) using pre-loaded Whisper model."""
        import tempfile
        import os

        if not getattr(self, '_whisper_model', None):
            logger.error("[STT] Whisper model not loaded")
            return None

        try:
            from infrastructure.config import Config
            provider = Config.get("stt.active_provider", "whisper_local")
            language = Config.get(f"stt.providers.{provider}.language", "en")

            logger.debug(f"[STT] Starting transcription: backend={self._whisper_backend}, language={language}")

            # Save audio bytes to temp file for transcription
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
                f.write(audio_bytes)
                temp_path = f.name

            try:
                start = time.time()
                logger.debug(f"[STT] Calling transcribe on temp file: {temp_path}")
                
                if self._whisper_backend == "faster_whisper":
                    logger.debug("[STT] Using faster-whisper backend")
                    segments, info = self._whisper_model.transcribe(temp_path, language=language)
                    text = " ".join([seg.text for seg in segments]).strip()
                    logger.debug(f"[STT] faster-whisper completed, detected language: {info.language}")
                elif "openai" in self._whisper_backend:
                    logger.debug("[STT] Using openai-whisper backend")
                    result = self._whisper_model.transcribe(temp_path, language=language, fp16=False)
                    text = result["text"].strip() if isinstance(result, dict) else str(result).strip()
                    logger.debug("[STT] openai-whisper completed")
                else:
                    logger.error(f"[STT] Unknown backend: {self._whisper_backend}")
                    return None

                duration = (time.time() - start) * 1000
                logger.info(f"[STT] Transcribed in {duration:.0f}ms: {text}")
                return text if text else None
            finally:
                if os.path.exists(temp_path):
                    os.unlink(temp_path)

        except Exception as e:
            logger.error(f"[STT] Transcription failed: {e}")
            import traceback
            logger.error(f"[STT] Full traceback: {traceback.format_exc()}")
            
            # NO CPU FALLBACK - Enforce GPU usage
            if "cublas" in str(e) or "cuda" in str(e).lower():
                logger.error("[STT] ═══════════════════════════════════════")
                logger.error("[STT] CUDA ERROR - Install proper PyTorch:")
                logger.error("[STT] pip uninstall torch torchvision torchaudio")
                logger.error("[STT] pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
                logger.error("[STT] ═══════════════════════════════════════")
            return None

    # ------------------------------------------------------------------
    # State API
    # ------------------------------------------------------------------

    def get_state(self) -> Dict[str, Any]:
        """Get the full system state for WebSocket broadcast."""
        return {
            "system_status": self._system_status,
            "pipeline": {k: dict(v) for k, v in self._pipeline_state.items()},
            "latest_description": self._latest_description,
            "spatial_summary": self._spatial_summary,
            "current_query": self._current_query,
            "current_response": self._current_response,
            "dialogue_history": self._dialogue_history[-20:],  # last 20
            "muted": self._muted,
        }

    def get_annotated_frame(self) -> Optional[bytes]:
        """Get the latest JPEG-encoded annotated frame."""
        with self._frame_lock:
            return self._latest_frame

    # ------------------------------------------------------------------
    # Main processing loop
    # ------------------------------------------------------------------

    def _main_loop(self):
        """Background thread running the pipeline."""
        logger.info("[SystemManager] Main loop started")
        self._pipeline_state["camera"]["active"] = True

        try:
            for frame in self._camera.stream():
                if not self._running:
                    break

                current_time = time.time()

                # --- YOLO Detection ---
                self._pipeline_state["yolo"]["active"] = True
                yolo_start = time.time()
                detections = self._detector.detect(frame)
                yolo_ms = (time.time() - yolo_start) * 1000
                self._pipeline_state["yolo"]["last_latency_ms"] = round(yolo_ms, 1)
                self._pipeline_state["yolo"]["detections_count"] = len(detections)
                self._detections = detections

                # Keep clean frame for VLM
                clean_frame = frame.copy()

                # Draw detections on display frame
                frame = self._draw_detections(frame, detections)

                # --- Safety Layer ---
                safety_result = self._safety.evaluate(detections)
                if safety_result:
                    alert_type, message = safety_result
                    self._pipeline_state["safety"]["active"] = True
                    self._pipeline_state["safety"]["last_alert"] = message
                    self._fusion.handle_safety_alert(message, alert_type)
                    self._latest_description = f"ALERT: {message}"
                else:
                    self._pipeline_state["safety"]["active"] = False
                    self._pipeline_state["safety"]["last_alert"] = None

                # --- Spatial Tracking ---
                self._fusion.update_spatial_context(detections, current_time, frame.shape[1])
                self._spatial_summary = self._fusion.get_spatial_summary()

                # --- Check for pending user queries ---
                try:
                    query = self._query_queue.get_nowait()
                    self._current_query = query
                    self._pipeline_state["stt"]["active"] = True

                    # Per architecture: just set pending_query on FusionEngine
                    # LLM only runs AFTER VLM provides scene description
                    self._fusion.pending_query = query
                    logger.info(f"[Pipeline] Query pending for VLM grounding: {query}")

                    self._pipeline_state["stt"]["active"] = False
                except queue.Empty:
                    pass

                # --- Harvest LLM results from VLM-grounded queries (non-blocking) ---
                llm_result = self._llm_worker.get_result()
                if llm_result:
                    result_type, answer, llm_ms = llm_result
                    self._pipeline_state["llm"]["last_latency_ms"] = round(llm_ms, 1)
                    self._pipeline_state["llm"]["is_processing"] = False

                    if answer:
                        self._current_response = answer
                        self._dialogue_history.append({
                            "role": "ai",
                            "text": answer,
                            "timestamp": time.time()
                        })
                        self._pipeline_state["tts"]["active"] = True
                        self._pipeline_state["tts"]["is_speaking"] = True

                    self._current_query = None

                # --- VLM Processing ---
                is_critical = safety_result and safety_result[0] == "CRITICAL_ALERT"
                if not is_critical:
                    # Harvest VLM results
                    result = self._qwen_worker.get_result()
                    if result:
                        new_desc, duration = result
                        self._pipeline_state["vlm"]["is_processing"] = False
                        self._pipeline_state["vlm"]["last_latency_ms"] = round(duration * 1000, 1)
                        self._latest_description = new_desc

                        if self._current_query:
                            # Submit VLM-grounded LLM query (non-blocking)
                            self._llm_worker.process_vlm_query(new_desc)
                            self._pipeline_state["llm"]["active"] = True
                            self._pipeline_state["llm"]["is_processing"] = True
                        else:
                            self._fusion.handle_scene_description(new_desc)

                    # Trigger VLM
                    has_query = self._current_query is not None
                    time_to_sample = self._sampler.should_sample()
                    should_run = False

                    if has_query:
                        should_run = True
                    elif time_to_sample and self._scene_detector.has_changed(clean_frame):
                        should_run = True

                    if should_run:
                        context_str = ", ".join([d["label"] for d in detections])
                        if has_query:
                            context_str += f". USER QUESTION: {self._current_query}"

                        if self._qwen_worker.process(clean_frame, context_str):
                            self._pipeline_state["vlm"]["active"] = True
                            self._pipeline_state["vlm"]["is_processing"] = True

                # --- Encode frame for MJPEG ---
                _, jpeg = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 80])
                with self._frame_lock:
                    self._latest_frame = jpeg.tobytes()

                # Throttle to ~30 FPS max
                time.sleep(0.001)

        except Exception as e:
            logger.error(f"[SystemManager] Main loop error: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self._running = False
            self._system_status = "IDLE"
            self._pipeline_state["camera"]["active"] = False
            if self._camera:
                self._camera.release()
                self._camera = None  # Allow re-init on next start
            logger.info("[SystemManager] Main loop ended")

    # ------------------------------------------------------------------
    # Drawing helpers
    # ------------------------------------------------------------------

    @staticmethod
    def _draw_detections(frame, detections: list) -> Any:
        """Draw YOLO bounding boxes on the frame."""
        for d in detections:
            x1, y1, x2, y2 = map(int, d["bbox"][0])
            label = d["label"]
            conf = d["confidence"]

            # Color by danger level
            lo = label.lower()
            if lo in {"knife", "gun", "fire", "stairs", "car", "bus", "truck", "bike"}:
                color = (0, 0, 255)
            elif lo in {"person", "dog", "animal", "bicycle", "wall", "glass"}:
                color = (0, 255, 255)
            elif lo in {"chair", "table", "bag"}:
                color = (255, 0, 0)
            else:
                color = (0, 255, 0)

            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            text = f"{label} {conf:.2f}"
            cv2.rectangle(frame, (x1, y1 - 22), (x1 + len(text) * 9, y1), color, -1)
            cv2.putText(frame, text, (x1 + 2, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)
        return frame


class _QwenWorker:
    """Async VLM worker — runs VLM inference in a background thread."""

    def __init__(self, qwen_instance):
        self.qwen = qwen_instance
        self.input_queue = queue.Queue(maxsize=1)
        self.output_queue = queue.Queue()
        self.running = True
        self.is_busy = False
        self.thread = threading.Thread(target=self._run, daemon=True)
        self.thread.start()

    def _run(self):
        while self.running:
            try:
                item = self.input_queue.get(timeout=0.1)
                frame, context_str = item
                self.is_busy = True
                try:
                    start = time.time()
                    desc = self.qwen.describe_scene(frame, context=context_str)
                    duration = time.time() - start
                    self.output_queue.put((desc, duration))
                except Exception as e:
                    logger.error(f"[QwenWorker] {e}")
                finally:
                    self.is_busy = False
                    self.input_queue.task_done()
            except queue.Empty:
                continue

    def process(self, frame, context_str: str) -> bool:
        if not self.is_busy and self.input_queue.empty():
            self.input_queue.put((frame.copy(), context_str))
            return True
        return False

    def get_result(self):
        try:
            return self.output_queue.get_nowait()
        except queue.Empty:
            return None

    def stop(self):
        self.running = False


class _LLMWorker:
    """Async LLM worker — runs LLM queries in a background thread.

    Accepts two types of requests:
    - 'query': Immediate LLM response to user query (handle_user_query)
    - 'vlm_query': VLM-grounded LLM response (handle_vlm_description)

    Results are tuples of (result_type, answer_text, latency_ms).
    """

    def __init__(self, fusion_engine):
        self.fusion = fusion_engine
        self.input_queue = queue.Queue(maxsize=1)
        self.output_queue = queue.Queue()
        self.running = True
        self.is_busy = False
        self.thread = threading.Thread(target=self._run, daemon=True)
        self.thread.start()

    def _run(self):
        while self.running:
            try:
                item = self.input_queue.get(timeout=0.1)
                request_type, payload = item
                self.is_busy = True
                try:
                    start = time.time()
                    if request_type == "query":
                        answer = self.fusion.handle_user_query(payload)
                        result_type = "immediate"
                    elif request_type == "vlm_query":
                        answer = self.fusion.handle_vlm_description(payload)
                        result_type = "vlm_grounded"
                    else:
                        answer = None
                        result_type = "unknown"

                    latency_ms = (time.time() - start) * 1000
                    self.output_queue.put((result_type, answer, latency_ms))
                except Exception as e:
                    logger.error(f"[LLMWorker] {e}")
                    import traceback
                    traceback.print_exc()
                finally:
                    self.is_busy = False
                    self.input_queue.task_done()
            except queue.Empty:
                continue

    def process_query(self, query: str) -> bool:
        """Submit a user query for immediate LLM response."""
        if not self.is_busy and self.input_queue.empty():
            self.input_queue.put(("query", query))
            return True
        return False

    def process_vlm_query(self, vlm_description: str) -> bool:
        """Submit a VLM-grounded query for LLM reasoning."""
        if not self.is_busy and self.input_queue.empty():
            self.input_queue.put(("vlm_query", vlm_description))
            return True
        return False

    def get_result(self):
        """Non-blocking: get completed LLM result, or None."""
        try:
            return self.output_queue.get_nowait()
        except queue.Empty:
            return None

    def stop(self):
        self.running = False
